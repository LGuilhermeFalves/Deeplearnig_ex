{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"Ex1/Exerc%C3%ADcio%201/ex1/","title":"Exerc\u00edcio 1","text":""},{"location":"Ex1/Exerc%C3%ADcio%201/ex1/#exercicio-1-separabilidade-de-classes","title":"Exerc\u00edcio 1 \u2014 Separabilidade de classes","text":""},{"location":"Ex1/Exerc%C3%ADcio%201/ex1/#analise-da-distribuicao-e-sobreposicao-das-classes","title":"An\u00e1lise da Distribui\u00e7\u00e3o e Sobreposi\u00e7\u00e3o das Classes","text":"<p>Foi criado o conjunto de dados sint\u00e9tico com 4 classes seguindo os seguintes par\u00e2metros:</p> <ul> <li>Classe 0 (m\u00e9dia [2,3], desvio [0.8,2.5]): Distribu\u00edda pr\u00f3xima ao canto inferior esquerdo, com dispers\u00e3o maior no eixo y.</li> <li>Classe 1 (m\u00e9dia [5,6], desvio [1.2,1.9]): Localizada mais ao centro.</li> <li>Classe 2 (m\u00e9dia [8,1], desvio [0.9,0.9]): Mais isolada.</li> <li>Classe 3 (m\u00e9dia [15,4], desvio [0.5,2.0]): Bastante afastada das outras classes, com dispers\u00e3o maior no eixo y.</li> </ul> <p></p>"},{"location":"Ex1/Exerc%C3%ADcio%201/ex1/#separacao-linear","title":"Separa\u00e7\u00e3o Linear","text":"<p>Ao vizualizar o gr\u00e1fico \u00e9 not\u00e1vel que n\u00e3o \u00e9 poss\u00edvel separar todas as classes com limites lineares simples, apesar da classe 3 estar afastada, h\u00e1 sobreposi\u00e7\u00e3o de dados entre as classes 0 e 1 oque impossibilitaria a separa\u00e7\u00e3o.</p>"},{"location":"Ex1/Exerc%C3%ADcio%201/ex1/#limites-de-decisao-decision-boundaries","title":"Limites de Decis\u00e3o (Decision Boundaries)","text":"<p>Baseado no gr\u00e1fico, os limites de decis\u00e3o que uma rede neural treinada poderia aprender seriam:</p> <ul> <li>Limites curvos ou n\u00e3o-lineares entre as classes 0 e 1, devido \u00e0 sobreposi\u00e7\u00e3o.</li> <li>Limites mais simples entre as classes 2 e 3.</li> </ul>"},{"location":"Ex1/Exerc%C3%ADcio%201/ex1/#codigo-com-gracao-de-dados","title":"C\u00f3digo com gra\u00e7\u00e3o de dados:","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\n# Par\u00e2metros das classes\nparams = [\n    {'mean': [2, 3], 'std': [0.8, 2.5]},\n    {'mean': [5, 6], 'std': [1.2, 1.9]},\n    {'mean': [8, 1], 'std': [0.9, 0.9]},\n    {'mean': [15, 4], 'std': [0.5, 2.0]}\n]\n\nnum_classes = 4\nsamples_per_class = 100\n\nX = []\ny = []\n\nfor i, p in enumerate(params):\n    x_class = np.random.normal(loc=p['mean'], scale=p['std'], size=(samples_per_class, 2))\n    X.append(x_class)\n    y.append(np.full(samples_per_class, i))\n\nX = np.vstack(X)\ny = np.concatenate(y)\n\ncolors = ['red', 'blue', 'green', 'purple']\nlabels = ['Classe 0', 'Classe 1', 'Classe 2', 'Classe 3']\n\nplt.figure(figsize=(8, 6))\nfor i in range(num_classes):\n    plt.scatter(X[y == i, 0], X[y == i, 1], c=colors[i], label=labels[i], alpha=0.7)\n\nplt.xlabel('X1')\nplt.ylabel('X2')\nplt.title('Distribui\u00e7\u00e3o das classes sint\u00e9ticas (Exerc\u00edcio 1)')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Gr\u00e1fico com limites de decis\u00e3o ilustrativos\nplt.figure(figsize=(8, 6))\nfor i in range(num_classes):\n    plt.scatter(X[y == i, 0], X[y == i, 1], c=colors[i], label=labels[i], alpha=0.7)\n\n# Limites de decis\u00e3o aproximados (apenas ilustrativos)\n# Entre classe 0 e 1 (reta)\nplt.plot([3.8, 4.2], [12.5, -3], 'k--', label='Limite 0/1')\n# Entre classe 1 e 2 (reta)\nplt.plot([4.1, 12.5], [2.8, 5.2], 'k-.', label='Limite 1/2')\n# Entre classe 2 e 3 (reta)\nplt.plot([11.30, 13.6], [11.1, -2], 'k:', label='Limite 2/3')\n\nplt.xlabel('X1')\nplt.ylabel('X2')\nplt.title('Distribui\u00e7\u00e3o das classes com limites de decis\u00e3o (Exerc\u00edcio 1)')\nplt.legend()\nplt.grid(True)\nplt.show()\n</code></pre>"},{"location":"Ex1/Exerc%C3%ADcio%202/ex2/","title":"Exerc\u00edcio 2","text":""},{"location":"Ex1/Exerc%C3%ADcio%202/ex2/#exercicio-2-nao-linearidade-em-dimensoes-superiores","title":"Exerc\u00edcio 2 \u2014 N\u00e3o-linearidade em Dimens\u00f5es Superiores","text":""},{"location":"Ex1/Exerc%C3%ADcio%202/ex2/#geracao-dos-dados","title":"Gera\u00e7\u00e3o dos Dados","text":"<p>Os dados foram gerados usando distribui\u00e7\u00f5es normais multivariadas conforme os seguintes par\u00e2metros:</p> <ul> <li>Classe A: m\u00e9dia <code>[0, 0, 0, 0, 0]</code>, matriz de covari\u00e2ncia.</li> <li>Classe B: m\u00e9dia <code>[1.5, 1.5, 1.5, 1.5, 1.5]</code>, matriz de covari\u00e2ncia.</li> </ul>"},{"location":"Ex1/Exerc%C3%ADcio%202/ex2/#reducao-de-dimensionalidade-e-visualizacao","title":"Redu\u00e7\u00e3o de Dimensionalidade e Visualiza\u00e7\u00e3o","text":"<p>Foi utilizada a t\u00e9cnica de An\u00e1lise de Componentes Principais (PCA) para projetar os dados de 5 dimens\u00f5es em 2D. O gr\u00e1fico abaixo mostra a proje\u00e7\u00e3o dos dados:</p> <p></p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\n# Par\u00e2metros Classe A\nmean_A = [0, 0, 0, 0, 0]\ncov_A = [\n    [1.0, 0.8, 0.1, 0.0, 0.0],\n    [0.8, 1.0, 0.3, 0.0, 0.0],\n    [0.1, 0.3, 1.0, 0.5, 0.0],\n    [0.0, 0.0, 0.5, 1.0, 0.2],\n    [0.0, 0.0, 0.0, 0.2, 1.0]\n]\nmean_B = [1.5, 1.5, 1.5, 1.5, 1.5]\ncov_B = [\n    [1.5, -0.7, 0.2, 0.0, 0.0],\n    [-0.7, 1.5, 0.4, 0.0, 0.0],\n    [0.2, 0.4, 1.5, 0.6, 0.0],\n    [0.0, 0.0, 0.6, 1.5, 0.3],\n    [0.0, 0.0, 0.0, 0.3, 1.5]\n]\nn_samples = 500\n\nX_A = np.random.multivariate_normal(mean_A, cov_A, n_samples)\nX_B = np.random.multivariate_normal(mean_B, cov_B, n_samples)\nX = np.vstack([X_A, X_B])\ny = np.array([0]*n_samples + [1]*n_samples)\n\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X)\n\nplt.figure(figsize=(8,6))\nplt.scatter(X_pca[y==0, 0], X_pca[y==0, 1], c='blue', label='Classe A', alpha=0.6)\nplt.scatter(X_pca[y==1, 0], X_pca[y==1, 1], c='red', label='Classe B', alpha=0.6)\nplt.xlabel('PCA 1')\nplt.ylabel('PCA 2')\nplt.title('Proje\u00e7\u00e3o PCA dos dados 5D para 2D')\nplt.legend()\nplt.grid(True)\nplt.show()\n</code></pre> <p>A proje\u00e7\u00e3o PCA mostra que as classes A e B possuem regi\u00f5es de sobreposi\u00e7\u00e3o e n\u00e3o s\u00e3o linearmente separ\u00e1veis no espa\u00e7o 2D projetado. Isso indica que, mesmo no espa\u00e7o original 5D, um modelo linear simples teria dificuldade em separar as classes com precis\u00e3o. Para esse tipo de estrutura de dados, redes neurais profundas com fun\u00e7\u00f5es de ativa\u00e7\u00e3o n\u00e3o-lineares s\u00e3o mais adequadas, pois conseguem aprender limites de decis\u00e3o complexos e n\u00e3o-lineares.</p>"},{"location":"Ex1/Exerc%C3%ADcio%203/ex3/","title":"Exerc\u00edcio 3","text":""},{"location":"Ex1/Exerc%C3%ADcio%203/ex3/#exercicio-3-preparando-dados-reais-para-rede-neural","title":"Exerc\u00edcio 3 \u2014 Preparando Dados Reais para Rede Neural","text":""},{"location":"Ex1/Exerc%C3%ADcio%203/ex3/#obtencao-dos-dados","title":"Obten\u00e7\u00e3o dos Dados","text":"<p>Para esse exerc\u00edcio, foi utilizado o dataset Spaceship Titanic, dispon\u00edvel no Kaggle, foi utilizado AI para aux\u00edlio no c\u00f3digo.</p>"},{"location":"Ex1/Exerc%C3%ADcio%203/ex3/#descrevendo-o-conjunto-de-dados","title":"Descrevendo o Conjunto de Dados","text":"<p>O dataset tem como objetivo prever se um passageiro foi transportado para outra dimens\u00e3o (coluna <code>Transported</code>, valores True/False).</p>"},{"location":"Ex1/Exerc%C3%ADcio%203/ex3/#principais-features","title":"Principais features","text":"<ul> <li>Num\u00e9ricas: <code>Age</code>, <code>RoomService</code>, <code>FoodCourt</code>, <code>ShoppingMall</code>, <code>Spa</code>, <code>VRDeck</code> e a derivada <code>CabinNum</code> (extra\u00edda de <code>Cabin</code>).</li> <li>Categ\u00f3ricas: <code>HomePlanet</code>, <code>CryoSleep</code>, <code>Destination</code>, <code>VIP</code> e, a partir de <code>Cabin</code>, as novas <code>Deck</code> e <code>Side</code>.</li> <li>Identificadores (removidos de X): <code>PassengerId</code>, <code>Name</code> (mantidos apenas como metadados).</li> </ul>"},{"location":"Ex1/Exerc%C3%ADcio%203/ex3/#codigo","title":"C\u00f3digo","text":"<pre><code>import os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nimport joblib\n\nTRAIN = os.path.join(\".\", \"train.csv\")\nTEST = os.path.join(\".\", \"test.csv\")\n\ntrain_df = pd.read_csv(TRAIN)\ntest_df = pd.read_csv(TEST)\n\n# Keep raw for plots\ntrain_raw = train_df.copy()\n\ntarget_col = \"Transported\" if \"Transported\" in train_df.columns else None\nid_like = [c for c in [\"PassengerId\", \"Name\"] if c in train_df.columns]\n\ndef split_cabin(df):\n    if \"Cabin\" in df.columns:\n        parts = df[\"Cabin\"].astype(str).str.split(\"/\", expand=True)\n        if parts.shape[1] &gt;= 3:\n            df[\"Deck\"] = parts[0].replace(\"nan\", np.nan)\n            df[\"CabinNum\"] = pd.to_numeric(parts[1].replace(\"nan\", np.nan), errors=\"coerce\")\n            df[\"Side\"] = parts[2].replace(\"nan\", np.nan)\n        elif parts.shape[1] == 2:\n            df[\"Deck\"] = parts[0].replace(\"nan\", np.nan)\n            df[\"CabinNum\"] = pd.to_numeric(parts[1].replace(\"nan\", np.nan), errors=\"coerce\")\n            df[\"Side\"] = np.nan\n        else:\n            df[\"Deck\"], df[\"CabinNum\"], df[\"Side\"] = np.nan, np.nan, np.nan\n        df.drop(columns=[\"Cabin\"], inplace=True)\n</code></pre> A coluna Cabin vem no formato Deck/CabinNum/Side (ex.: \u201cB/19/P\u201d). Foi divido essa string em tr\u00eas vari\u00e1veis: Deck (categ\u00f3rica), CabinNum (num\u00e9rica) e Side (categ\u00f3rica). Quando faltam partes, \u00e9 preenchido com NaN e substituido \"nan\" por NaN. Por fim, foi retirado a Cabin original para reduzir cardinalidade e ficar com atributos mais f\u00e1ceis de codificar e escalar. <pre><code>split_cabin(train_df)\nsplit_cabin(test_df)\n\npreferred_numeric = [c for c in [\"Age\",\"RoomService\",\"FoodCourt\",\"ShoppingMall\",\"Spa\",\"VRDeck\",\"CabinNum\"] if c in train_df.columns]\nauto_numeric = [c for c in train_df.select_dtypes(include=[np.number]).columns if c != target_col]\nnum_cols = sorted(list(set(preferred_numeric) | set(auto_numeric)))\n\ncat_cols = [c for c in train_df.columns if (train_df[c].dtype == \"object\" or train_df[c].dtype == \"bool\")]\ncat_cols = [c for c in cat_cols if c not in (set(id_like) | {target_col})]\n\nfeature_cols = [c for c in train_df.columns if c not in (set(id_like) | {target_col})]\n\n# Missing values table\nmissing_counts = train_raw.isnull().sum().reset_index()\nmissing_counts.columns = [\"column\",\"n_missing\"]\nmissing_counts[\"pct_missing\"] = (missing_counts[\"n_missing\"] / len(train_raw) * 100).round(2)\nmissing_counts.to_csv(\"missing_values_train.csv\", index=False)\n\n# Preprocess pipeline\nnumeric_transformer = Pipeline(\n    steps=[(\"imputer\", SimpleImputer(strategy=\"median\")),\n           (\"scaler\", StandardScaler())]\n)\ncategorical_transformer = Pipeline(\n    steps=[(\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n           (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse=False))]\n)\npreprocess = ColumnTransformer(\n    transformers=[(\"num\", numeric_transformer, num_cols),\n                  (\"cat\", categorical_transformer, cat_cols)]\n)\n\nX_train = train_df[feature_cols].copy()\nX_test = test_df[feature_cols].copy()\ny_train = train_df[target_col].astype(int) if target_col else None\n\nX_train_proc = preprocess.fit_transform(X_train)\nX_test_proc = preprocess.transform(X_test)\n\nnum_names = num_cols\ncat_names = list(preprocess.named_transformers_[\"cat\"].named_steps[\"onehot\"].get_feature_names_out(cat_cols))\nproc_columns = list(num_names) + list(cat_names)\n\nX_train_proc_df = pd.DataFrame(X_train_proc, columns=proc_columns, index=train_df.index)\nX_test_proc_df = pd.DataFrame(X_test_proc, columns=proc_columns, index=test_df.index)\n\nX_train_proc_df.to_csv(\"processed_train_features.csv\", index=False)\nX_test_proc_df.to_csv(\"processed_test_features.csv\", index=False)\nif y_train is not None:\n    pd.Series(y_train, name=target_col).to_csv(\"processed_train_labels.csv\", index=False)\n</code></pre> Foi aplicado o split_cabin ao train e ao test para substituir Cabin por Deck, CabinNum e Side, ap\u00f3s isso foi definido as colunas num\u00e9ricas combinando uma lista \u201cpreferida\u201d (Age, gastos e CabinNum) com a detec\u00e7\u00e3o autom\u00e1tica de colunas de tipo num\u00e9rico, e definido as categ\u00f3ricas como as de texto/booleanas, excluindo identificadores (PassengerId, Name) e o alvo (Transported). A lista feature_cols re\u00fane todas as vari\u00e1veis de entrada (tudo menos IDs e alvo), isso foi salvo em missing_values_train.csv, ap\u00f3s a prepara\u00e7\u00e3o dos dados foram salvos os arquivos processed_train_features.csv, processed_test_features.csv e, se houver alvo, processed_train_labels.csv. <pre><code>joblib.dump({\"pipeline\": preprocess,\n             \"feature_cols\": feature_cols,\n             \"num_cols\": num_cols,\n             \"cat_cols\": cat_cols}, \"preprocess_pipeline.joblib\")\n\n# Plots\nif \"Age\" in train_raw.columns:\n    plt.figure()\n    train_raw[\"Age\"].dropna().plot(kind=\"hist\", bins=30)\n    plt.title(\"Age - Antes do scaling\")\n    plt.xlabel(\"Age (original)\")\n    plt.ylabel(\"Frequ\u00eancia\")\n    plt.savefig(\"age_before.png\", bbox_inches=\"tight\")\n    plt.close()\n\n    age_idx = num_cols.index(\"Age\")\n    age_scaled = X_train_proc[:, age_idx]\n    plt.figure()\n    pd.Series(age_scaled).plot(kind=\"hist\", bins=30)\n    plt.title(\"Age - Depois do scaling (StandardScaler)\")\n    plt.xlabel(\"Age (escalado)\")\n    plt.ylabel(\"Frequ\u00eancia\")\n    plt.savefig(\"age_after.png\", bbox_inches=\"tight\")\n    plt.close()\n\ncandidates = [c for c in [\"FoodCourt\",\"VRDeck\",\"RoomService\",\"Spa\",\"ShoppingMall\"] if c in train_raw.columns]\nif candidates:\n    second_feat = candidates[0]\n    plt.figure()\n    train_raw[second_feat].dropna().plot(kind=\"hist\", bins=30)\n    plt.title(f\"{second_feat} - Antes do scaling\")\n    plt.xlabel(f\"{second_feat} (original)\")\n    plt.ylabel(\"Frequ\u00eancia\")\n    plt.savefig(f\"{second_feat.lower()}_before.png\", bbox_inches=\"tight\")\n    plt.close()\n\n    idx = num_cols.index(second_feat)\n    scaled_vals = X_train_proc[:, idx]\n    plt.figure()\n    pd.Series(scaled_vals).plot(kind=\"hist\", bins=30)\n    plt.title(f\"{second_feat} - Depois do scaling (StandardScaler)\")\n    plt.xlabel(f\"{second_feat} (escalado)\")\n    plt.ylabel(\"Frequ\u00eancia\")\n    plt.savefig(f\"{second_feat.lower()}_after.png\", bbox_inches=\"tight\")\n    plt.close()\n</code></pre> <p>Foi salvo pipeline e metadados em preprocess_pipeline.joblib para reuso e reprodu\u00e7\u00e3o das mesmas transforma\u00e7\u00f5es em valida\u00e7\u00e3o/produ\u00e7\u00e3o. Depois, foi gerado histogramas para Age e para  FoodCourt, mostrando antes e depois do StandardScaler.</p>"},{"location":"Ex1/Exerc%C3%ADcio%203/ex3/#visualizacoes-antes-depois-do-scaling","title":"Visualiza\u00e7\u00f5es (antes \u00d7 depois do scaling)","text":"<ul> <li> <p>Age: </p> </li> <li> <p>FoodCourt: </p> </li> </ul> <p>Antes da padroniza\u00e7\u00e3o, as vari\u00e1veis apresentaram diferentes escalas e distribui\u00e7\u00f5es, o que pode dificultar o treinamento da rede neural. Ap\u00f3s o scaling, os dados ficaram centrados e com vari\u00e2ncia padronizada,</p>"},{"location":"Ex1/Exerc%C3%ADcio%203/ex3/#saidas-geradas","title":"Sa\u00eddas Geradas","text":"<ul> <li><code>processed_train_features.csv</code> \u2014 features do treino j\u00e1 imputadas, one-hot e escaladas.</li> <li><code>processed_train_labels.csv</code> \u2014 r\u00f3tulos do treino (<code>Transported</code> \u2192 0/1).</li> <li><code>processed_test_features.csv</code> \u2014 features do teste processadas com o mesmo pipeline.</li> <li><code>preprocess_pipeline.joblib</code> \u2014 pipeline do scikit-learn para reuso e reprodutibilidade.</li> </ul>"},{"location":"Ex2_Perceptron/Exerc%C3%ADcio%204/perception1/","title":"Exerc\u00edcio 4","text":""},{"location":"Ex2_Perceptron/Exerc%C3%ADcio%204/perception1/#exercicio-1-perception","title":"Exerc\u00edcio 1 \u2014 Perception","text":""},{"location":"Ex2_Perceptron/Exerc%C3%ADcio%204/perception1/#geracao-de-dados","title":"Gera\u00e7\u00e3o de dados","text":"<p>Os dados foram gerados seguindo os seguintes par\u00e2metros</p> <ul> <li>Classe 0 </li> <li>M\u00e9dia: <code>[1.5, 1.5]</code> </li> <li> <p>Matriz de covari\u00e2ncia: <code>[[0.5, 0], [0, 0.5]]</code> </p> </li> <li> <p>Classe 1 </p> </li> <li>M\u00e9dia: <code>[5, 5]</code> </li> <li>Matriz de covari\u00e2ncia: <code>[[0.5, 0], [0, 0.5]]</code> </li> </ul> <p></p>"},{"location":"Ex2_Perceptron/Exerc%C3%ADcio%204/perception1/#implementacao-do-perceptron","title":"Implementa\u00e7\u00e3o do Perceptron","text":"<p>O perceptron foi implementado do zero em Python. A regra de atualiza\u00e7\u00e3o utilizada foi:</p> \\[ w = w + n*y*x and b = b + n*y \\] <p>O treinamento ocorreu at\u00e9 converg\u00eancia ou at\u00e9 100 \u00e9pocas.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\n\nnp.random.seed(42)\n\nn_samples = 1000\nmean0 = [1.5, 1.5]\ncov0 = [[0.5, 0], [0, 0.5]]\nmean1 = [5, 5]\ncov1 = [[0.5, 0], [0, 0.5]]\n\n# Classe 0\nX0 = np.random.multivariate_normal(mean0, cov0, n_samples)\ny0 = np.zeros(n_samples)\n\n# Classe 1\nX1 = np.random.multivariate_normal(mean1, cov1, n_samples)\ny1 = np.ones(n_samples)\n\n# Concatena\nX = np.vstack((X0, X1))\ny = np.hstack((y0, y1))\n\n# Embaralha\nidx = np.arange(len(y))\nnp.random.shuffle(idx)\nX, y = X[idx], y[idx]\n\n\nclass Perceptron:\n    def __init__(self, lr=0.01, epochs=100):\n        self.lr = lr\n        self.epochs = epochs\n        self.w = None\n        self.b = 0\n        self.accuracies = []\n\n    def fit(self, X, y):\n        n_samples, n_features = X.shape\n        self.w = np.zeros(n_features)\n        self.b = 0\n\n        # Converte labels para -1 e +1\n        y_mod = np.where(y == 1, 1, -1)\n\n        for epoch in range(self.epochs):\n            errors = 0\n            for xi, yi in zip(X, y_mod):\n                linear_output = np.dot(xi, self.w) + self.b\n                y_pred = 1 if linear_output &gt;= 0 else -1\n                if yi != y_pred:\n                    # Regra de atualiza\u00e7\u00e3o\n                    self.w += self.lr * yi * xi\n                    self.b += self.lr * yi\n                    errors += 1\n\n\n            y_pred_all = self.predict(X)\n            acc = np.mean(y_pred_all == y)\n            self.accuracies.append(acc)\n\n            if errors == 0:  # convergiu\n                break\n\n    def predict(self, X):\n        linear_output = np.dot(X, self.w) + self.b\n        return np.where(linear_output &gt;= 0, 1, 0)\n\n    perc = Perceptron(lr=0.01, epochs=100)\n    perc.fit(X, y)\n\n    y_pred = perc.predict(X)\n    accuracy = np.mean(y_pred == y)\n</code></pre> <p>Como as distribui\u00e7\u00f5es t\u00eam m\u00e9dias distantes e baixa vari\u00e2ncia, as classes s\u00e3o praticamente linearmente separ\u00e1veis, fazendo o perceptron convergir r\u00e1pido</p> <p></p>"},{"location":"Ex2_Perceptron/Exerc%C3%ADcio%205/perception2/","title":"Exerc\u00edcio 5","text":""},{"location":"Ex2_Perceptron/Exerc%C3%ADcio%205/perception2/#exercicio-2-perceptron","title":"Exerc\u00edcio 2 \u2014 Perceptron","text":""},{"location":"Ex2_Perceptron/Exerc%C3%ADcio%205/perception2/#geracao-de-dados","title":"Gera\u00e7\u00e3o de dados","text":"<p>Foram geradas 1000 amostras por classe,com os seguintes par\u00e2metros:</p> <ul> <li>Classe 0 </li> <li>M\u00e9dia: <code>[3, 3]</code> </li> <li> <p>Matriz de covari\u00e2ncia: <code>[[1.5, 0], [0, 1.5]]</code> </p> </li> <li> <p>Classe 1 </p> </li> <li>M\u00e9dia: <code>[4, 4]</code> </li> <li>Matriz de covari\u00e2ncia: <code>[[1.5, 0], [0, 1.5]]</code> </li> </ul> <p>Esses par\u00e2metros criam uma  parcial entre as classes, j\u00e1 que as m\u00e9dias est\u00e3o pr\u00f3ximas e a vari\u00e2ncia \u00e9 alta.</p> <p></p>"},{"location":"Ex2_Perceptron/Exerc%C3%ADcio%205/perception2/#implementacao-do-perceptron","title":"Implementa\u00e7\u00e3o do Perceptron","text":"<p>A implementa\u00e7\u00e3o foi a mesma do Exerc\u00edcio 1, utilizando a regra de atualiza\u00e7\u00e3o do perceptron com taxa de aprendizado n= 0.01, treinando at\u00e9 converg\u00eancia ou at\u00e9 100 \u00e9pocas.</p> <pre><code>class Perceptron:\n    def __init__(self, lr=0.01, epochs=100):\n        self.lr = lr\n        self.epochs = epochs\n        self.w = None\n        self.b = 0\n        self.accuracies = []\n\n    def fit(self, X, y):\n        n_samples, n_features = X.shape\n        self.w = np.zeros(n_features)\n        self.b = 0\n        y_mod = np.where(y == 1, 1, -1)\n\n        for epoch in range(self.epochs):\n            errors = 0\n            for xi, yi in zip(X, y_mod):\n                linear_output = np.dot(xi, self.w) + self.b\n                y_pred = 1 if linear_output &gt;= 0 else -1\n                if yi != y_pred:\n                    self.w += self.lr * yi * xi\n                    self.b += self.lr * yi\n                    errors += 1\n\n            y_pred_all = self.predict(X)\n            acc = np.mean(y_pred_all == y)\n            self.accuracies.append(acc)\n\n            if errors == 0:\n                break\n\n    def predict(self, X):\n        linear_output = np.dot(X, self.w) + self.b\n        return np.where(linear_output &gt;= 0, 1, 0)\n</code></pre> <p></p> <p>O perceptron n\u00e3o consegue convergir totalmente porque os dados n\u00e3o s\u00e3o linearmente separ\u00e1veis. A sobreposi\u00e7\u00e3o entre as duas distribui\u00e7\u00f5es (m\u00e9dias pr\u00f3ximas e vari\u00e2ncia maior) cria pontos que sempre estar\u00e3o no lado errado da fronteira</p> <p></p>"}]}