{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"Ex1/Exerc%C3%ADcio%201/ex1/","title":"Exerc\u00edcio 1","text":""},{"location":"Ex1/Exerc%C3%ADcio%201/ex1/#exercicio-1-separabilidade-de-classes","title":"Exerc\u00edcio 1 \u2014 Separabilidade de classes","text":""},{"location":"Ex1/Exerc%C3%ADcio%201/ex1/#analise-da-distribuicao-e-sobreposicao-das-classes","title":"An\u00e1lise da Distribui\u00e7\u00e3o e Sobreposi\u00e7\u00e3o das Classes","text":"<p>Foi criado o conjunto de dados sint\u00e9tico com 4 classes seguindo os seguintes par\u00e2metros:</p> <ul> <li>Classe 0 (m\u00e9dia [2,3], desvio [0.8,2.5]): Distribu\u00edda pr\u00f3xima ao canto inferior esquerdo, com dispers\u00e3o maior no eixo y.</li> <li>Classe 1 (m\u00e9dia [5,6], desvio [1.2,1.9]): Localizada mais ao centro.</li> <li>Classe 2 (m\u00e9dia [8,1], desvio [0.9,0.9]): Mais isolada.</li> <li>Classe 3 (m\u00e9dia [15,4], desvio [0.5,2.0]): Bastante afastada das outras classes, com dispers\u00e3o maior no eixo y.</li> </ul> <p></p>"},{"location":"Ex1/Exerc%C3%ADcio%201/ex1/#separacao-linear","title":"Separa\u00e7\u00e3o Linear","text":"<p>Ao vizualizar o gr\u00e1fico \u00e9 not\u00e1vel que n\u00e3o \u00e9 poss\u00edvel separar todas as classes com limites lineares simples, apesar da classe 3 estar afastada, h\u00e1 sobreposi\u00e7\u00e3o de dados entre as classes 0 e 1 oque impossibilitaria a separa\u00e7\u00e3o.</p>"},{"location":"Ex1/Exerc%C3%ADcio%201/ex1/#limites-de-decisao-decision-boundaries","title":"Limites de Decis\u00e3o (Decision Boundaries)","text":"<p>Baseado no gr\u00e1fico, os limites de decis\u00e3o que uma rede neural treinada poderia aprender seriam:</p> <ul> <li>Limites curvos ou n\u00e3o-lineares entre as classes 0 e 1, devido \u00e0 sobreposi\u00e7\u00e3o.</li> <li>Limites mais simples entre as classes 2 e 3.</li> </ul>"},{"location":"Ex1/Exerc%C3%ADcio%201/ex1/#codigo-com-gracao-de-dados","title":"C\u00f3digo com gra\u00e7\u00e3o de dados:","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\n# Par\u00e2metros das classes\nparams = [\n    {'mean': [2, 3], 'std': [0.8, 2.5]},\n    {'mean': [5, 6], 'std': [1.2, 1.9]},\n    {'mean': [8, 1], 'std': [0.9, 0.9]},\n    {'mean': [15, 4], 'std': [0.5, 2.0]}\n]\n\nnum_classes = 4\nsamples_per_class = 100\n\nX = []\ny = []\n\nfor i, p in enumerate(params):\n    x_class = np.random.normal(loc=p['mean'], scale=p['std'], size=(samples_per_class, 2))\n    X.append(x_class)\n    y.append(np.full(samples_per_class, i))\n\nX = np.vstack(X)\ny = np.concatenate(y)\n\ncolors = ['red', 'blue', 'green', 'purple']\nlabels = ['Classe 0', 'Classe 1', 'Classe 2', 'Classe 3']\n\nplt.figure(figsize=(8, 6))\nfor i in range(num_classes):\n    plt.scatter(X[y == i, 0], X[y == i, 1], c=colors[i], label=labels[i], alpha=0.7)\n\nplt.xlabel('X1')\nplt.ylabel('X2')\nplt.title('Distribui\u00e7\u00e3o das classes sint\u00e9ticas (Exerc\u00edcio 1)')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Gr\u00e1fico com limites de decis\u00e3o ilustrativos\nplt.figure(figsize=(8, 6))\nfor i in range(num_classes):\n    plt.scatter(X[y == i, 0], X[y == i, 1], c=colors[i], label=labels[i], alpha=0.7)\n\n# Limites de decis\u00e3o aproximados (apenas ilustrativos)\n# Entre classe 0 e 1 (reta)\nplt.plot([3.8, 4.2], [12.5, -3], 'k--', label='Limite 0/1')\n# Entre classe 1 e 2 (reta)\nplt.plot([4.1, 12.5], [2.8, 5.2], 'k-.', label='Limite 1/2')\n# Entre classe 2 e 3 (reta)\nplt.plot([11.30, 13.6], [11.1, -2], 'k:', label='Limite 2/3')\n\nplt.xlabel('X1')\nplt.ylabel('X2')\nplt.title('Distribui\u00e7\u00e3o das classes com limites de decis\u00e3o (Exerc\u00edcio 1)')\nplt.legend()\nplt.grid(True)\nplt.show()\n</code></pre>"},{"location":"Ex1/Exerc%C3%ADcio%202/ex2/","title":"Exerc\u00edcio 2","text":""},{"location":"Ex1/Exerc%C3%ADcio%202/ex2/#exercicio-2-nao-linearidade-em-dimensoes-superiores","title":"Exerc\u00edcio 2 \u2014 N\u00e3o-linearidade em Dimens\u00f5es Superiores","text":""},{"location":"Ex1/Exerc%C3%ADcio%202/ex2/#geracao-dos-dados","title":"Gera\u00e7\u00e3o dos Dados","text":"<p>Os dados foram gerados usando distribui\u00e7\u00f5es normais multivariadas conforme os seguintes par\u00e2metros:</p> <ul> <li>Classe A: m\u00e9dia <code>[0, 0, 0, 0, 0]</code>, matriz de covari\u00e2ncia.</li> <li>Classe B: m\u00e9dia <code>[1.5, 1.5, 1.5, 1.5, 1.5]</code>, matriz de covari\u00e2ncia.</li> </ul>"},{"location":"Ex1/Exerc%C3%ADcio%202/ex2/#reducao-de-dimensionalidade-e-visualizacao","title":"Redu\u00e7\u00e3o de Dimensionalidade e Visualiza\u00e7\u00e3o","text":"<p>Foi utilizada a t\u00e9cnica de An\u00e1lise de Componentes Principais (PCA) para projetar os dados de 5 dimens\u00f5es em 2D. O gr\u00e1fico abaixo mostra a proje\u00e7\u00e3o dos dados:</p> <p></p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\n# Par\u00e2metros Classe A\nmean_A = [0, 0, 0, 0, 0]\ncov_A = [\n    [1.0, 0.8, 0.1, 0.0, 0.0],\n    [0.8, 1.0, 0.3, 0.0, 0.0],\n    [0.1, 0.3, 1.0, 0.5, 0.0],\n    [0.0, 0.0, 0.5, 1.0, 0.2],\n    [0.0, 0.0, 0.0, 0.2, 1.0]\n]\nmean_B = [1.5, 1.5, 1.5, 1.5, 1.5]\ncov_B = [\n    [1.5, -0.7, 0.2, 0.0, 0.0],\n    [-0.7, 1.5, 0.4, 0.0, 0.0],\n    [0.2, 0.4, 1.5, 0.6, 0.0],\n    [0.0, 0.0, 0.6, 1.5, 0.3],\n    [0.0, 0.0, 0.0, 0.3, 1.5]\n]\nn_samples = 500\n\nX_A = np.random.multivariate_normal(mean_A, cov_A, n_samples)\nX_B = np.random.multivariate_normal(mean_B, cov_B, n_samples)\nX = np.vstack([X_A, X_B])\ny = np.array([0]*n_samples + [1]*n_samples)\n\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X)\n\nplt.figure(figsize=(8,6))\nplt.scatter(X_pca[y==0, 0], X_pca[y==0, 1], c='blue', label='Classe A', alpha=0.6)\nplt.scatter(X_pca[y==1, 0], X_pca[y==1, 1], c='red', label='Classe B', alpha=0.6)\nplt.xlabel('PCA 1')\nplt.ylabel('PCA 2')\nplt.title('Proje\u00e7\u00e3o PCA dos dados 5D para 2D')\nplt.legend()\nplt.grid(True)\nplt.show()\n</code></pre> <p>A proje\u00e7\u00e3o PCA mostra que as classes A e B possuem regi\u00f5es de sobreposi\u00e7\u00e3o e n\u00e3o s\u00e3o linearmente separ\u00e1veis no espa\u00e7o 2D projetado. Isso indica que, mesmo no espa\u00e7o original 5D, um modelo linear simples teria dificuldade em separar as classes com precis\u00e3o. Para esse tipo de estrutura de dados, redes neurais profundas com fun\u00e7\u00f5es de ativa\u00e7\u00e3o n\u00e3o-lineares s\u00e3o mais adequadas, pois conseguem aprender limites de decis\u00e3o complexos e n\u00e3o-lineares.</p>"},{"location":"Ex1/Exerc%C3%ADcio%203/ex3/","title":"Exerc\u00edcio 3","text":""},{"location":"Ex1/Exerc%C3%ADcio%203/ex3/#exercicio-3-preparando-dados-reais-para-rede-neural","title":"Exerc\u00edcio 3 \u2014 Preparando Dados Reais para Rede Neural","text":""},{"location":"Ex1/Exerc%C3%ADcio%203/ex3/#obtencao-dos-dados","title":"Obten\u00e7\u00e3o dos Dados","text":"<p>Para esse exerc\u00edcio, foi utilizado o dataset Spaceship Titanic, dispon\u00edvel no Kaggle, foi utilizado AI para aux\u00edlio no c\u00f3digo.</p>"},{"location":"Ex1/Exerc%C3%ADcio%203/ex3/#descrevendo-o-conjunto-de-dados","title":"Descrevendo o Conjunto de Dados","text":"<p>O dataset tem como objetivo prever se um passageiro foi transportado para outra dimens\u00e3o (coluna <code>Transported</code>, valores True/False).</p>"},{"location":"Ex1/Exerc%C3%ADcio%203/ex3/#principais-features","title":"Principais features","text":"<ul> <li>Num\u00e9ricas: <code>Age</code>, <code>RoomService</code>, <code>FoodCourt</code>, <code>ShoppingMall</code>, <code>Spa</code>, <code>VRDeck</code> e a derivada <code>CabinNum</code> (extra\u00edda de <code>Cabin</code>).</li> <li>Categ\u00f3ricas: <code>HomePlanet</code>, <code>CryoSleep</code>, <code>Destination</code>, <code>VIP</code> e, a partir de <code>Cabin</code>, as novas <code>Deck</code> e <code>Side</code>.</li> <li>Identificadores (removidos de X): <code>PassengerId</code>, <code>Name</code> (mantidos apenas como metadados).</li> </ul>"},{"location":"Ex1/Exerc%C3%ADcio%203/ex3/#codigo","title":"C\u00f3digo","text":"<pre><code>import os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nimport joblib\n\nTRAIN = os.path.join(\".\", \"train.csv\")\nTEST = os.path.join(\".\", \"test.csv\")\n\ntrain_df = pd.read_csv(TRAIN)\ntest_df = pd.read_csv(TEST)\n\n# Keep raw for plots\ntrain_raw = train_df.copy()\n\ntarget_col = \"Transported\" if \"Transported\" in train_df.columns else None\nid_like = [c for c in [\"PassengerId\", \"Name\"] if c in train_df.columns]\n\ndef split_cabin(df):\n    if \"Cabin\" in df.columns:\n        parts = df[\"Cabin\"].astype(str).str.split(\"/\", expand=True)\n        if parts.shape[1] &gt;= 3:\n            df[\"Deck\"] = parts[0].replace(\"nan\", np.nan)\n            df[\"CabinNum\"] = pd.to_numeric(parts[1].replace(\"nan\", np.nan), errors=\"coerce\")\n            df[\"Side\"] = parts[2].replace(\"nan\", np.nan)\n        elif parts.shape[1] == 2:\n            df[\"Deck\"] = parts[0].replace(\"nan\", np.nan)\n            df[\"CabinNum\"] = pd.to_numeric(parts[1].replace(\"nan\", np.nan), errors=\"coerce\")\n            df[\"Side\"] = np.nan\n        else:\n            df[\"Deck\"], df[\"CabinNum\"], df[\"Side\"] = np.nan, np.nan, np.nan\n        df.drop(columns=[\"Cabin\"], inplace=True)\n</code></pre> A coluna Cabin vem no formato Deck/CabinNum/Side (ex.: \u201cB/19/P\u201d). Foi divido essa string em tr\u00eas vari\u00e1veis: Deck (categ\u00f3rica), CabinNum (num\u00e9rica) e Side (categ\u00f3rica). Quando faltam partes, \u00e9 preenchido com NaN e substituido \"nan\" por NaN. Por fim, foi retirado a Cabin original para reduzir cardinalidade e ficar com atributos mais f\u00e1ceis de codificar e escalar. <pre><code>split_cabin(train_df)\nsplit_cabin(test_df)\n\npreferred_numeric = [c for c in [\"Age\",\"RoomService\",\"FoodCourt\",\"ShoppingMall\",\"Spa\",\"VRDeck\",\"CabinNum\"] if c in train_df.columns]\nauto_numeric = [c for c in train_df.select_dtypes(include=[np.number]).columns if c != target_col]\nnum_cols = sorted(list(set(preferred_numeric) | set(auto_numeric)))\n\ncat_cols = [c for c in train_df.columns if (train_df[c].dtype == \"object\" or train_df[c].dtype == \"bool\")]\ncat_cols = [c for c in cat_cols if c not in (set(id_like) | {target_col})]\n\nfeature_cols = [c for c in train_df.columns if c not in (set(id_like) | {target_col})]\n\n# Missing values table\nmissing_counts = train_raw.isnull().sum().reset_index()\nmissing_counts.columns = [\"column\",\"n_missing\"]\nmissing_counts[\"pct_missing\"] = (missing_counts[\"n_missing\"] / len(train_raw) * 100).round(2)\nmissing_counts.to_csv(\"missing_values_train.csv\", index=False)\n\n# Preprocess pipeline\nnumeric_transformer = Pipeline(\n    steps=[(\"imputer\", SimpleImputer(strategy=\"median\")),\n           (\"scaler\", StandardScaler())]\n)\ncategorical_transformer = Pipeline(\n    steps=[(\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n           (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse=False))]\n)\npreprocess = ColumnTransformer(\n    transformers=[(\"num\", numeric_transformer, num_cols),\n                  (\"cat\", categorical_transformer, cat_cols)]\n)\n\nX_train = train_df[feature_cols].copy()\nX_test = test_df[feature_cols].copy()\ny_train = train_df[target_col].astype(int) if target_col else None\n\nX_train_proc = preprocess.fit_transform(X_train)\nX_test_proc = preprocess.transform(X_test)\n\nnum_names = num_cols\ncat_names = list(preprocess.named_transformers_[\"cat\"].named_steps[\"onehot\"].get_feature_names_out(cat_cols))\nproc_columns = list(num_names) + list(cat_names)\n\nX_train_proc_df = pd.DataFrame(X_train_proc, columns=proc_columns, index=train_df.index)\nX_test_proc_df = pd.DataFrame(X_test_proc, columns=proc_columns, index=test_df.index)\n\nX_train_proc_df.to_csv(\"processed_train_features.csv\", index=False)\nX_test_proc_df.to_csv(\"processed_test_features.csv\", index=False)\nif y_train is not None:\n    pd.Series(y_train, name=target_col).to_csv(\"processed_train_labels.csv\", index=False)\n</code></pre> Foi aplicado o split_cabin ao train e ao test para substituir Cabin por Deck, CabinNum e Side, ap\u00f3s isso foi definido as colunas num\u00e9ricas combinando uma lista \u201cpreferida\u201d (Age, gastos e CabinNum) com a detec\u00e7\u00e3o autom\u00e1tica de colunas de tipo num\u00e9rico, e definido as categ\u00f3ricas como as de texto/booleanas, excluindo identificadores (PassengerId, Name) e o alvo (Transported). A lista feature_cols re\u00fane todas as vari\u00e1veis de entrada (tudo menos IDs e alvo), isso foi salvo em missing_values_train.csv, ap\u00f3s a prepara\u00e7\u00e3o dos dados foram salvos os arquivos processed_train_features.csv, processed_test_features.csv e, se houver alvo, processed_train_labels.csv. <pre><code>joblib.dump({\"pipeline\": preprocess,\n             \"feature_cols\": feature_cols,\n             \"num_cols\": num_cols,\n             \"cat_cols\": cat_cols}, \"preprocess_pipeline.joblib\")\n\n# Plots\nif \"Age\" in train_raw.columns:\n    plt.figure()\n    train_raw[\"Age\"].dropna().plot(kind=\"hist\", bins=30)\n    plt.title(\"Age - Antes do scaling\")\n    plt.xlabel(\"Age (original)\")\n    plt.ylabel(\"Frequ\u00eancia\")\n    plt.savefig(\"age_before.png\", bbox_inches=\"tight\")\n    plt.close()\n\n    age_idx = num_cols.index(\"Age\")\n    age_scaled = X_train_proc[:, age_idx]\n    plt.figure()\n    pd.Series(age_scaled).plot(kind=\"hist\", bins=30)\n    plt.title(\"Age - Depois do scaling (StandardScaler)\")\n    plt.xlabel(\"Age (escalado)\")\n    plt.ylabel(\"Frequ\u00eancia\")\n    plt.savefig(\"age_after.png\", bbox_inches=\"tight\")\n    plt.close()\n\ncandidates = [c for c in [\"FoodCourt\",\"VRDeck\",\"RoomService\",\"Spa\",\"ShoppingMall\"] if c in train_raw.columns]\nif candidates:\n    second_feat = candidates[0]\n    plt.figure()\n    train_raw[second_feat].dropna().plot(kind=\"hist\", bins=30)\n    plt.title(f\"{second_feat} - Antes do scaling\")\n    plt.xlabel(f\"{second_feat} (original)\")\n    plt.ylabel(\"Frequ\u00eancia\")\n    plt.savefig(f\"{second_feat.lower()}_before.png\", bbox_inches=\"tight\")\n    plt.close()\n\n    idx = num_cols.index(second_feat)\n    scaled_vals = X_train_proc[:, idx]\n    plt.figure()\n    pd.Series(scaled_vals).plot(kind=\"hist\", bins=30)\n    plt.title(f\"{second_feat} - Depois do scaling (StandardScaler)\")\n    plt.xlabel(f\"{second_feat} (escalado)\")\n    plt.ylabel(\"Frequ\u00eancia\")\n    plt.savefig(f\"{second_feat.lower()}_after.png\", bbox_inches=\"tight\")\n    plt.close()\n</code></pre> <p>Foi salvo pipeline e metadados em preprocess_pipeline.joblib para reuso e reprodu\u00e7\u00e3o das mesmas transforma\u00e7\u00f5es em valida\u00e7\u00e3o/produ\u00e7\u00e3o. Depois, foi gerado histogramas para Age e para  FoodCourt, mostrando antes e depois do StandardScaler.</p>"},{"location":"Ex1/Exerc%C3%ADcio%203/ex3/#visualizacoes-antes-depois-do-scaling","title":"Visualiza\u00e7\u00f5es (antes \u00d7 depois do scaling)","text":"<ul> <li> <p>Age: </p> </li> <li> <p>FoodCourt: </p> </li> </ul> <p>Antes da padroniza\u00e7\u00e3o, as vari\u00e1veis apresentaram diferentes escalas e distribui\u00e7\u00f5es, o que pode dificultar o treinamento da rede neural. Ap\u00f3s o scaling, os dados ficaram centrados e com vari\u00e2ncia padronizada,</p>"},{"location":"Ex1/Exerc%C3%ADcio%203/ex3/#saidas-geradas","title":"Sa\u00eddas Geradas","text":"<ul> <li><code>processed_train_features.csv</code> \u2014 features do treino j\u00e1 imputadas, one-hot e escaladas.</li> <li><code>processed_train_labels.csv</code> \u2014 r\u00f3tulos do treino (<code>Transported</code> \u2192 0/1).</li> <li><code>processed_test_features.csv</code> \u2014 features do teste processadas com o mesmo pipeline.</li> <li><code>preprocess_pipeline.joblib</code> \u2014 pipeline do scikit-learn para reuso e reprodutibilidade.</li> </ul>"},{"location":"Ex2_Perceptron/Exerc%C3%ADcio%204/perception1/","title":"Exerc\u00edcio 4","text":""},{"location":"Ex2_Perceptron/Exerc%C3%ADcio%204/perception1/#exercicio-1-perception","title":"Exerc\u00edcio 1 \u2014 Perception","text":""},{"location":"Ex2_Perceptron/Exerc%C3%ADcio%204/perception1/#geracao-de-dados","title":"Gera\u00e7\u00e3o de dados","text":"<p>Os dados foram gerados seguindo os seguintes par\u00e2metros</p> <ul> <li>Classe 0 </li> <li>M\u00e9dia: <code>[1.5, 1.5]</code> </li> <li> <p>Matriz de covari\u00e2ncia: <code>[[0.5, 0], [0, 0.5]]</code> </p> </li> <li> <p>Classe 1 </p> </li> <li>M\u00e9dia: <code>[5, 5]</code> </li> <li>Matriz de covari\u00e2ncia: <code>[[0.5, 0], [0, 0.5]]</code> </li> </ul> <p></p>"},{"location":"Ex2_Perceptron/Exerc%C3%ADcio%204/perception1/#implementacao-do-perceptron","title":"Implementa\u00e7\u00e3o do Perceptron","text":"<p>O perceptron foi implementado do zero em Python. A regra de atualiza\u00e7\u00e3o utilizada foi:</p> \\[ w = w + n*y*x and b = b + n*y \\] <p>O treinamento ocorreu at\u00e9 converg\u00eancia ou at\u00e9 100 \u00e9pocas.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\n\nnp.random.seed(42)\n\nn_samples = 1000\nmean0 = [1.5, 1.5]\ncov0 = [[0.5, 0], [0, 0.5]]\nmean1 = [5, 5]\ncov1 = [[0.5, 0], [0, 0.5]]\n\n# Classe 0\nX0 = np.random.multivariate_normal(mean0, cov0, n_samples)\ny0 = np.zeros(n_samples)\n\n# Classe 1\nX1 = np.random.multivariate_normal(mean1, cov1, n_samples)\ny1 = np.ones(n_samples)\n\n# Concatena\nX = np.vstack((X0, X1))\ny = np.hstack((y0, y1))\n\n# Embaralha\nidx = np.arange(len(y))\nnp.random.shuffle(idx)\nX, y = X[idx], y[idx]\n\n\nclass Perceptron:\n    def __init__(self, lr=0.01, epochs=100):\n        self.lr = lr\n        self.epochs = epochs\n        self.w = None\n        self.b = 0\n        self.accuracies = []\n\n    def fit(self, X, y):\n        n_samples, n_features = X.shape\n        self.w = np.zeros(n_features)\n        self.b = 0\n\n        # Converte labels para -1 e +1\n        y_mod = np.where(y == 1, 1, -1)\n\n        for epoch in range(self.epochs):\n            errors = 0\n            for xi, yi in zip(X, y_mod):\n                linear_output = np.dot(xi, self.w) + self.b\n                y_pred = 1 if linear_output &gt;= 0 else -1\n                if yi != y_pred:\n                    # Regra de atualiza\u00e7\u00e3o\n                    self.w += self.lr * yi * xi\n                    self.b += self.lr * yi\n                    errors += 1\n\n\n            y_pred_all = self.predict(X)\n            acc = np.mean(y_pred_all == y)\n            self.accuracies.append(acc)\n\n            if errors == 0:  # convergiu\n                break\n\n    def predict(self, X):\n        linear_output = np.dot(X, self.w) + self.b\n        return np.where(linear_output &gt;= 0, 1, 0)\n\n    perc = Perceptron(lr=0.01, epochs=100)\n    perc.fit(X, y)\n\n    y_pred = perc.predict(X)\n    accuracy = np.mean(y_pred == y)\n</code></pre> <p>Como as distribui\u00e7\u00f5es t\u00eam m\u00e9dias distantes e baixa vari\u00e2ncia, as classes s\u00e3o praticamente linearmente separ\u00e1veis, fazendo o perceptron convergir r\u00e1pido</p> <p></p>"},{"location":"Ex2_Perceptron/Exerc%C3%ADcio%205/perception2/","title":"Exerc\u00edcio 5","text":""},{"location":"Ex2_Perceptron/Exerc%C3%ADcio%205/perception2/#exercicio-2-perceptron","title":"Exerc\u00edcio 2 \u2014 Perceptron","text":""},{"location":"Ex2_Perceptron/Exerc%C3%ADcio%205/perception2/#geracao-de-dados","title":"Gera\u00e7\u00e3o de dados","text":"<p>Foram geradas 1000 amostras por classe,com os seguintes par\u00e2metros:</p> <ul> <li>Classe 0 </li> <li>M\u00e9dia: <code>[3, 3]</code> </li> <li> <p>Matriz de covari\u00e2ncia: <code>[[1.5, 0], [0, 1.5]]</code> </p> </li> <li> <p>Classe 1 </p> </li> <li>M\u00e9dia: <code>[4, 4]</code> </li> <li>Matriz de covari\u00e2ncia: <code>[[1.5, 0], [0, 1.5]]</code> </li> </ul> <p>Esses par\u00e2metros criam uma  parcial entre as classes, j\u00e1 que as m\u00e9dias est\u00e3o pr\u00f3ximas e a vari\u00e2ncia \u00e9 alta.</p> <p></p>"},{"location":"Ex2_Perceptron/Exerc%C3%ADcio%205/perception2/#implementacao-do-perceptron","title":"Implementa\u00e7\u00e3o do Perceptron","text":"<p>A implementa\u00e7\u00e3o foi a mesma do Exerc\u00edcio 1, utilizando a regra de atualiza\u00e7\u00e3o do perceptron com taxa de aprendizado n= 0.01, treinando at\u00e9 converg\u00eancia ou at\u00e9 100 \u00e9pocas.</p> <pre><code>class Perceptron:\n    def __init__(self, lr=0.01, epochs=100):\n        self.lr = lr\n        self.epochs = epochs\n        self.w = None\n        self.b = 0\n        self.accuracies = []\n\n    def fit(self, X, y):\n        n_samples, n_features = X.shape\n        self.w = np.zeros(n_features)\n        self.b = 0\n        y_mod = np.where(y == 1, 1, -1)\n\n        for epoch in range(self.epochs):\n            errors = 0\n            for xi, yi in zip(X, y_mod):\n                linear_output = np.dot(xi, self.w) + self.b\n                y_pred = 1 if linear_output &gt;= 0 else -1\n                if yi != y_pred:\n                    self.w += self.lr * yi * xi\n                    self.b += self.lr * yi\n                    errors += 1\n\n            y_pred_all = self.predict(X)\n            acc = np.mean(y_pred_all == y)\n            self.accuracies.append(acc)\n\n            if errors == 0:\n                break\n\n    def predict(self, X):\n        linear_output = np.dot(X, self.w) + self.b\n        return np.where(linear_output &gt;= 0, 1, 0)\n</code></pre> <p></p> <p>O perceptron n\u00e3o consegue convergir totalmente porque os dados n\u00e3o s\u00e3o linearmente separ\u00e1veis. A sobreposi\u00e7\u00e3o entre as duas distribui\u00e7\u00f5es (m\u00e9dias pr\u00f3ximas e vari\u00e2ncia maior) cria pontos que sempre estar\u00e3o no lado errado da fronteira</p> <p></p>"},{"location":"Ex3_Mlp/Ex6/mlp/","title":"Exerc\u00edcio 6","text":""},{"location":"Ex3_Mlp/Ex6/mlp/#exercicio-1-mlp","title":"Exerc\u00edcio 1 \u2014 Mlp","text":"<p>Nessa atividade foi feita a implementa\u00e7\u00e3o passo a passo (forward + backprop) para uma MLP com 2 entradas, 1 camada oculta com 2 neur\u00f4nios e 1 sa\u00edda. A ativa\u00e7\u00e3o usada \u00e9 tanh e a atualiza\u00e7\u00e3o de par\u00e2metros foi feita com learning rate n = 0.3.</p>"},{"location":"Ex3_Mlp/Ex6/mlp/#codigo-python","title":"C\u00f3digo (Python)","text":"<pre><code>import numpy as np\n\n# Dados fornecidos\nx = np.array([0.5, -0.2], dtype=float)\ny = 1.0\n\nW1 = np.array([[0.3, -0.1],\n               [0.2,  0.4]], dtype=float)\nb1 = np.array([0.1, -0.2], dtype=float)\n\nW2 = np.array([0.5, -0.3], dtype=float)\nb2 = 0.2\n\nn = 0.3  # learning rate (ajustado)\n\ndef tanh(u):\n    return np.tanh(u)\n\ndef dtanh(u):\n    return 1.0 - np.tanh(u)**2\n\n# Forward pass\nu1 = W1.dot(x) + b1\nh1 = tanh(u1)\nu2 = W2.dot(h1) + b2\ny_hat = tanh(u2)\nL = 0.5 * (y - y_hat)**2\n\n# Backprop\ndL_dyhat = -(y - y_hat)\ndyhat_du2 = dtanh(u2)\ndelta2 = dL_dyhat * dyhat_du2\n\ndL_dW2 = delta2 * h1\ndL_db2 = delta2\n\ndL_dh1 = delta2 * W2\ndh1_du1 = dtanh(u1)\ndelta1 = dL_dh1 * dh1_du1\n\ndL_dW1 = np.outer(delta1, x)\ndL_db1 = delta1\n\n# Updates (gradient descent) with n = 0.3\nW2_new = W2 - n * dL_dW2\nb2_new = b2 - n * dL_db2\n\nW1_new = W1 - n * dL_dW1\nb1_new = b1 - n * dL_db1\n\n# Print results\nnp.set_printoptions(precision=6, suppress=True)\nprint(\"Forward pass:\")\nprint(\"u1 =\", u1)\nprint(\"h1 =\", h1)\nprint(\"u2 =\", u2)\nprint(\"y_hat =\", y_hat)\nprint(\"Loss L =\", L)\n\nprint(\"\\nGradients:\")\nprint(\"dL/dW2 =\", dL_dW2)\nprint(\"dL/db2 =\", dL_db2)\nprint(\"dL/dW1 =\\n\", dL_dW1)\nprint(\"dL/db1 =\", dL_db1)\n\nprint(\"\\nParameter updates (n=0.3):\")\nprint(\"W2_old =\", W2)\nprint(\"W2_new =\", W2_new)\nprint(\"b2_old =\", b2)\nprint(\"b2_new =\", b2_new)\nprint(\"W1_old =\\n\", W1)\nprint(\"W1_new =\\n\", W1_new)\nprint(\"b1_old =\", b1)\nprint(\"b1_new =\", b1_new)\n</code></pre>"},{"location":"Ex3_Mlp/Ex6/mlp/#saidas-resultados-numericos","title":"Sa\u00eddas (resultados num\u00e9ricos)","text":"<p>Forward pass: </p><pre><code>u1 = [ 0.27 -0.18]\nh1 = [ 0.263625 -0.178081]\nu2 = 0.38523667817130075\ny_hat = 0.36724656264510797\nLoss L = 0.2001884562422156\n</code></pre><p></p> <p>Gradients: </p><pre><code>dL/dW2 = [-0.144312  0.097484]\ndL/db2 = -0.5474139573567998\ndL/dW1 =\n [[-0.127342  0.050937]\n [ 0.079508 -0.031803]]\ndL/db1 = [-0.254685  0.159016]\n</code></pre><p></p> <p>Parameter updates (n=0.3): </p><pre><code>W2_old = [ 0.5 -0.3]\nW2_new = [ 0.543294 -0.329245]\nb2_old = 0.2\nb2_new = 0.36422418720704\n\nW1_old =\n [[ 0.3 -0.1]\n [ 0.2  0.4]]\nW1_new =\n [[ 0.338203 -0.115281]\n [ 0.176148  0.409541]]\n\nb1_old = [ 0.1 -0.2]\nb1_new = [ 0.176405 -0.247705]\n</code></pre><p></p>"},{"location":"Ex3_Mlp/Ex6/mlp/#breve-explicacao","title":"Breve explica\u00e7\u00e3o","text":"<ul> <li>Forward: calcula pre-ativa\u00e7\u00e3o e ativa\u00e7\u00e3o na camada oculta (<code>u1</code>, <code>h1</code>), em seguida a pre-ativa\u00e7\u00e3o e ativa\u00e7\u00e3o de sa\u00edda (<code>u2</code>, <code>y_hat</code>) e a perda MSE.</li> <li>Backprop: computa <code>delta2</code> (gradiente no n\u00f3 de sa\u00edda), depois os gradientes de <code>W2</code> e <code>b2</code>, propaga para a camada oculta (obtendo <code>delta1</code>) e calcula gradientes de <code>W1</code> e <code>b1</code>.</li> <li>Atualiza\u00e7\u00e3o: aplica gradiente descendente com <code>n = 0.3</code> para obter os novos par\u00e2metros.</li> </ul>"},{"location":"Ex3_Mlp/Ex7/mlp2/","title":"Exerc\u00edcio 7","text":""},{"location":"Ex3_Mlp/Ex7/mlp2/#exercise-2","title":"Exercise 2","text":"<p>Este exerc\u00edcio implementa um Perceptron Multicamadas (MLP) do zero utilizando apenas NumPy, com o objetivo de classificar dados bidimensionais sint\u00e9ticos gerados pela fun\u00e7\u00e3o make_classification. O prop\u00f3sito \u00e9 praticar manualmente as etapas de propaga\u00e7\u00e3o direta (forward pass), retropropaga\u00e7\u00e3o do erro (backpropagation) e atualiza\u00e7\u00e3o dos pesos via gradiente descendente, sem o uso de frameworks como TensorFlow ou PyTorch.</p>"},{"location":"Ex3_Mlp/Ex7/mlp2/#codigo-python","title":"C\u00f3digo (Python)","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n\nnp.random.seed(42)\nX_all, y_all = make_classification(\n    n_samples=1000,\n    n_features=2,\n    n_informative=2,\n    n_redundant=0,\n    n_repeated=0,\n    n_classes=3,\n    n_clusters_per_class=1,\n    class_sep=1.25,\n    flip_y=0.03,\n    random_state=42\n)\ny_mapped = np.where(y_all == 0, 0, 1)\n\n# ---------------------------\n# 2) Train/test split + standardize\n# ---------------------------\nX_train, X_test, y_train, y_test = train_test_split(\n    X_all, y_mapped, test_size=0.2, random_state=42, stratify=y_mapped\n)\nmean = X_train.mean(axis=0)\nstd = X_train.std(axis=0)\nX_train_s = (X_train - mean) / std\nX_test_s = (X_test - mean) / std\n\n# ---------------------------\n# 3) MLP implementation (simple, from scratch)\n# ---------------------------\nclass SimpleMLP:\n    def __init__(self, n_inputs, n_hidden=8, lr=0.05, seed=1):\n        rng = np.random.RandomState(seed)\n        self.lr = lr\n        self.W1 = rng.normal(scale=0.5, size=(n_hidden, n_inputs))\n        self.b1 = np.zeros(n_hidden)\n        self.W2 = rng.normal(scale=0.5, size=(1, n_hidden))\n        self.b2 = 0.0\n\n    def sigmoid(self, z):\n        return 1.0 / (1.0 + np.exp(-z))\n\n    def dtanh(self, z):\n        return 1.0 - np.tanh(z)**2\n\n    def forward(self, X):\n        z1 = X.dot(self.W1.T) + self.b1\n        a1 = np.tanh(z1)\n        z2 = a1.dot(self.W2.T) + self.b2\n        a2 = self.sigmoid(z2).reshape(-1)\n        cache = (z1, a1, z2, a2)\n        return a2, cache\n\n    def compute_loss(self, y_true, y_pred):\n        eps = 1e-9\n        y_pred = np.clip(y_pred, eps, 1-eps)\n        return -np.mean(y_true * np.log(y_pred) + (1-y_true) * np.log(1-y_pred))\n\n    def backward(self, X, y, cache):\n        z1, a1, z2, a2 = cache\n        N = X.shape[0]\n        dz2 = (a2 - y) / N\n        dW2 = dz2.reshape(-1,1).T.dot(a1)\n        db2 = dz2.sum()\n        da1 = dz2.reshape(-1,1).dot(self.W2)\n        dz1 = da1 * self.dtanh(z1)\n        dW1 = dz1.T.dot(X)\n        db1 = dz1.sum(axis=0)\n        return dW1, db1, dW2, db2\n\n# ---------------------------\n# 4) Training loop with outputs\n# ---------------------------\nmodel = SimpleMLP(n_inputs=2, n_hidden=8, lr=0.05, seed=1)\nepochs = 300\ntrain_losses = []\nprint(\"Starting training...\")\nfor epoch in range(1, epochs+1):\n    y_pred_train, cache = model.forward(X_train_s)\n    loss = model.compute_loss(y_train, y_pred_train)\n    train_losses.append(loss)\n    dW1, db1, dW2, db2 = model.backward(X_train_s, y_train, cache)\n    model.W1 -= model.lr * dW1\n    model.b1 -= model.lr * db1\n    model.W2 -= model.lr * dW2\n    model.b2 -= model.lr * db2\n    if epoch == 1 or epoch % 25 == 0:\n        train_acc = accuracy_score(y_train, (y_pred_train&gt;=0.5).astype(int))\n        print(f\"Epoch {epoch:03d} - loss: {loss:.4f} - train_acc: {train_acc:.4f}\")\n\nprint(\"Training finished.\")\n\ny_pred_test, _ = model.forward(X_test_s)\ny_pred_test_labels = (y_pred_test &gt;= 0.5).astype(int)\ntest_acc = accuracy_score(y_test, y_pred_test_labels)\ncm = confusion_matrix(y_test, y_pred_test_labels)\n\nprint(\"\\nTest Results:\")\nprint(f\"Test accuracy: {test_acc:.4f}\")\nprint(\"Confusion matrix:\")\nprint(cm)\nprint(\"\\nClassification report:\")\nprint(classification_report(y_test, y_pred_test_labels, digits=4))\n</code></pre>"},{"location":"Ex3_Mlp/Ex7/mlp2/#saidas-resultados","title":"Sa\u00eddas (resultados)","text":"<p>Training progress: </p><pre><code>Epoch 001 - loss: 0.5524 - train_acc: 0.8338\nEpoch 025 - loss: 0.3812 - train_acc: 0.8938\nEpoch 050 - loss: 0.3594 - train_acc: 0.9012\nEpoch 100 - loss: 0.3011 - train_acc: 0.9087\nEpoch 200 - loss: 0.2564 - train_acc: 0.9163\nEpoch 300 - loss: 0.2374 - train_acc: 0.9187\nTraining finished.\n</code></pre><p></p> <p>Test results: </p><pre><code>Test accuracy: 0.9150\nConfusion matrix:\n[[ 63   3]\n [ 14 120]]\nClassification report:\n              precision    recall  f1-score   support\n\n           0     0.8182    0.9545    0.8814        66\n           1     0.9756    0.8955    0.9340       134\n\n    accuracy                         0.9150       200\n   macro avg     0.8969    0.9250    0.9077       200\nweighted avg     0.9213    0.9150    0.9165       200\n</code></pre><p></p>"},{"location":"Ex3_Mlp/Ex7/mlp2/#visualizacoes","title":"Visualiza\u00e7\u00f5es","text":""},{"location":"Ex3_Mlp/Ex7/mlp2/#test-set-true-vs-predicted-labels","title":"Test set \u2014 True vs Predicted labels","text":""},{"location":"Ex3_Mlp/Ex7/mlp2/#decision-boundary","title":"Decision Boundary","text":""},{"location":"Ex3_Mlp/Ex7/mlp2/#training-loss-curve","title":"Training Loss Curve","text":""},{"location":"Ex3_Mlp/Ex7/mlp2/#conclusao","title":"Conclus\u00e3o","text":"<p>O modelo atingiu 91.5% de acur\u00e1cia no conjunto de teste, mostrando que o perceptron multicamadas foi capaz de aprender fronteiras n\u00e3o lineares mesmo com apenas uma camada oculta de 8 neur\u00f4nios. A perda de treino diminuiu de forma suave e cont\u00ednua, indicando converg\u00eancia est\u00e1vel. As figuras mostram que a fronteira de decis\u00e3o separa bem as classes, e os erros s\u00e3o poucos e concentrados nas \u00e1reas de sobreposi\u00e7\u00e3o dos clusters.</p>"},{"location":"Ex3_Mlp/Ex8/mlp3/","title":"Exerc\u00edcio 8","text":""},{"location":"Ex3_Mlp/Ex8/mlp3/#exercicio-3-classificacao-multiclasse-com-mlp","title":"Exerc\u00edcio 3 \u2014 Classifica\u00e7\u00e3o Multiclasse com MLP","text":"<p>Nessa atividade \u00e9 feita a implementa\u00e7\u00e3o de um Perceptron Multicamadas (MLP) reutiliz\u00e1vel, baseado na estrutura do Exerc\u00edcio 2, agora para classifica\u00e7\u00e3o multiclasse (3 classes).  </p>"},{"location":"Ex3_Mlp/Ex8/mlp3/#codigo-python","title":"C\u00f3digo (Python)","text":"<pre><code># ex3_mlp_multiclass.py\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom sklearn.decomposition import PCA\n\nnp.random.seed(42)\nn_total = 1500\nper_class = n_total // 3\n\n\nX0, y0 = make_classification(\n    n_samples=per_class, n_features=4, n_informative=4, n_redundant=0,\n    n_repeated=0, n_classes=2, n_clusters_per_class=1,\n    class_sep=1.25, flip_y=0.02, random_state=1)\ny0 = np.zeros_like(y0)\n\nX1, y1 = make_classification(\n    n_samples=per_class, n_features=4, n_informative=4, n_redundant=0,\n    n_repeated=0, n_classes=3, n_clusters_per_class=1,\n    class_sep=1.25, flip_y=0.02, random_state=2)\ny1 = np.ones_like(y1)\n\nX2, y2 = make_classification(\n    n_samples=per_class, n_features=4, n_informative=4, n_redundant=0,\n    n_repeated=0, n_classes=4, n_clusters_per_class=1,\n    class_sep=1.25, flip_y=0.02, random_state=3)\ny2 = np.ones_like(y2) * 2\n\nX_all = np.vstack([X0, X1, X2])\ny_all = np.hstack([y0, y1, y2])\nperm = np.random.permutation(len(y_all))\nX_all, y_all = X_all[perm], y_all[perm]\n\n# Divis\u00e3o e normaliza\u00e7\u00e3o\nX_train, X_test, y_train, y_test = train_test_split(\n    X_all, y_all, test_size=0.2, random_state=42, stratify=y_all)\nmean, std = X_train.mean(axis=0), X_train.std(axis=0)\nX_train_s = (X_train - mean) / std\nX_test_s = (X_test - mean) / std\n\ndef one_hot(y, C):\n    yy = np.zeros((len(y), C))\n    yy[np.arange(len(y)), y.astype(int)] = 1\n    return yy\n\ny_train_oh = one_hot(y_train, 3)\ny_test_oh = one_hot(y_test, 3)\n\nclass SimpleMLPMulti:\n    def __init__(self, n_inputs, n_hidden=32, n_classes=3, lr=0.05, seed=1):\n        rng = np.random.RandomState(seed)\n        self.lr = lr\n        self.W1 = rng.normal(scale=0.5, size=(n_hidden, n_inputs))\n        self.b1 = np.zeros(n_hidden)\n        self.W2 = rng.normal(scale=0.5, size=(n_classes, n_hidden))\n        self.b2 = np.zeros(n_classes)\n\n    def tanh(self, z):\n        return np.tanh(z)\n\n    def dtanh(self, z):\n        return 1.0 - np.tanh(z)**2\n\n    def softmax(self, z):\n        z_max = np.max(z, axis=1, keepdims=True)\n        exp = np.exp(z - z_max)\n        return exp / exp.sum(axis=1, keepdims=True)\n\n    def forward(self, X):\n        z1 = X.dot(self.W1.T) + self.b1\n        a1 = self.tanh(z1)\n        z2 = a1.dot(self.W2.T) + self.b2\n        a2 = self.softmax(z2)\n        cache = (z1, a1, z2, a2)\n        return a2, cache\n\n    def compute_loss(self, y_true_oh, y_pred):\n        eps = 1e-9\n        y_pred = np.clip(y_pred, eps, 1-eps)\n        return -np.mean(np.sum(y_true_oh * np.log(y_pred), axis=1))\n\n    def backward(self, X, y_true_oh, cache):\n        z1, a1, z2, a2 = cache\n        N = X.shape[0]\n        dz2 = (a2 - y_true_oh) / N\n        dW2 = dz2.T.dot(a1)\n        db2 = dz2.sum(axis=0)\n        da1 = dz2.dot(self.W2)\n        dz1 = da1 * self.dtanh(z1)\n        dW1 = dz1.T.dot(X)\n        db1 = dz1.sum(axis=0)\n        return dW1, db1, dW2, db2\n\n\nmodel = SimpleMLPMulti(n_inputs=4, n_hidden=32, n_classes=3, lr=0.05, seed=1)\nepochs = 400\ntrain_losses = []\n\nfor epoch in range(1, epochs+1):\n    y_pred_train, cache = model.forward(X_train_s)\n    loss = model.compute_loss(y_train_oh, y_pred_train)\n    train_losses.append(loss)\n    dW1, db1, dW2, db2 = model.backward(X_train_s, y_train_oh, cache)\n    model.W1 -= model.lr * dW1\n    model.b1 -= model.lr * db1\n    model.W2 -= model.lr * dW2\n    model.b2 -= model.lr * db2\n    if epoch == 1 or epoch % 50 == 0:\n        train_acc = np.mean(np.argmax(y_pred_train, axis=1) == y_train)\n        print(f\"Epoch {epoch:03d} - loss: {loss:.4f} - train_acc: {train_acc:.4f}\")\n\n# Avalia\u00e7\u00e3o\ny_pred_test_prob, _ = model.forward(X_test_s)\ny_pred_test = np.argmax(y_pred_test_prob, axis=1)\ntest_acc = accuracy_score(y_test, y_pred_test)\ncm = confusion_matrix(y_test, y_pred_test)\nprint(\"\\nAcur\u00e1cia de teste:\", test_acc)\nprint(\"Matriz de confus\u00e3o:\\n\", cm)\n</code></pre>"},{"location":"Ex3_Mlp/Ex8/mlp3/#saidas-resultados","title":"Sa\u00eddas (resultados)","text":"<p>Progresso do treino: </p><pre><code>Epoch 001 - loss: 1.1881 - train_acc: 0.5750\nEpoch 050 - loss: 0.7357 - train_acc: 0.6700\nEpoch 100 - loss: 0.7049 - train_acc: 0.6883\nEpoch 200 - loss: 0.6600 - train_acc: 0.7058\nEpoch 300 - loss: 0.6239 - train_acc: 0.7308\nEpoch 400 - loss: 0.5930 - train_acc: 0.7508\n</code></pre><p></p> <p>Resultados no teste: </p><pre><code>Test accuracy: 0.7600\nConfusion matrix:\n[[81  9 10]\n [ 4 81 15]\n [ 8 26 66]]\n</code></pre><p></p>"},{"location":"Ex3_Mlp/Ex8/mlp3/#visualizacoes","title":"Visualiza\u00e7\u00f5es","text":""},{"location":"Ex3_Mlp/Ex8/mlp3/#projecao-pca-rotulos-verdadeiros","title":"Proje\u00e7\u00e3o PCA \u2014 R\u00f3tulos verdadeiros","text":""},{"location":"Ex3_Mlp/Ex8/mlp3/#projecao-pca-rotulos-preditos","title":"Proje\u00e7\u00e3o PCA \u2014 R\u00f3tulos preditos","text":""},{"location":"Ex3_Mlp/Ex8/mlp3/#curva-de-perda-durante-o-treino","title":"Curva de perda durante o treino","text":""},{"location":"Ex3_Mlp/Ex8/mlp3/#conclusao","title":"Conclus\u00e3o","text":"<p>O modelo MLP multiclasse atingiu 76% de acur\u00e1cia no conjunto de teste. Mesmo mantendo a estrutura simples do Exerc\u00edcio 2 (uma camada oculta e ativa\u00e7\u00e3o tanh), o modelo conseguiu capturar fronteiras n\u00e3o lineares entre as 3 classes geradas artificialmente. A curva de perda mostra uma converg\u00eancia est\u00e1vel, e as proje\u00e7\u00f5es em PCA confirmam que a separa\u00e7\u00e3o entre classes \u00e9 razo\u00e1vel, embora ainda exista sobreposi\u00e7\u00e3o \u2014 principalmente entre as classes 1 e 2.</p>"},{"location":"Ex3_Mlp/Ex9/mlp4/","title":"Exerc\u00edcio 9","text":""},{"location":"Ex3_Mlp/Ex9/mlp4/#exercicio-4-mlp-profunda-pelo-menos-2-camadas-ocultas","title":"Exerc\u00edcio 4 \u2014 MLP Profunda (pelo menos 2 camadas ocultas)","text":"<p>Este exerc\u00edcio adapta o Exerc\u00edcio 3 para uma rede MLP com pelo menos duas camadas ocultas. Reaproveitei a estrutura do exerc\u00edcio anterior e apenas estendi para duas camadas ocultas (arquitetura, forward, backprop e updates mantidos).</p>"},{"location":"Ex3_Mlp/Ex9/mlp4/#codigo-python","title":"C\u00f3digo (Python)","text":"<pre><code># ex4_mlp_multiclass_deep.py\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom sklearn.decomposition import PCA\n\n\nnp.random.seed(42)\nn_total = 1500\nper_class = n_total // 3\n\nX0, y0 = make_classification(n_samples=per_class, n_features=4, n_informative=4,\n    n_redundant=0, n_repeated=0, n_classes=2, n_clusters_per_class=1,\n    class_sep=1.25, flip_y=0.02, random_state=1)\ny0 = np.zeros_like(y0)\n\nX1, y1 = make_classification(n_samples=per_class, n_features=4, n_informative=4,\n    n_redundant=0, n_repeated=0, n_classes=3, n_clusters_per_class=1,\n    class_sep=1.25, flip_y=0.02, random_state=2)\ny1 = np.ones_like(y1)\n\nX2, y2 = make_classification(n_samples=per_class, n_features=4, n_informative=4,\n    n_redundant=0, n_repeated=0, n_classes=4, n_clusters_per_class=1,\n    class_sep=1.25, flip_y=0.02, random_state=3)\ny2 = np.ones_like(y2) * 2\n\nX_all = np.vstack([X0, X1, X2])\ny_all = np.hstack([y0, y1, y2])\nperm = np.random.permutation(len(y_all))\nX_all, y_all = X_all[perm], y_all[perm]\n\n# Split e padroniza\u00e7\u00e3o\nX_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2, random_state=42, stratify=y_all)\nmean = X_train.mean(axis=0); std = X_train.std(axis=0)\nX_train_s = (X_train - mean) / std; X_test_s = (X_test - mean) / std\n\ndef one_hot(y, C):\n    yy = np.zeros((len(y), C))\n    yy[np.arange(len(y)), y.astype(int)] = 1\n    return yy\n\ny_train_oh = one_hot(y_train, 3)\ny_test_oh = one_hot(y_test, 3)\n\n# MLP com 2 camadas ocultas (tanh), sa\u00edda softmax\nclass DeeperMLP:\n    def __init__(self, n_inputs, hidden_sizes=(48,24), n_classes=3, lr=0.05, seed=1):\n        rng = np.random.RandomState(seed)\n        self.lr = lr\n        h1, h2 = hidden_sizes\n        self.W1 = rng.normal(scale=0.5, size=(h1, n_inputs)); self.b1 = np.zeros(h1)\n        self.W2 = rng.normal(scale=0.5, size=(h2, h1)); self.b2 = np.zeros(h2)\n        self.W3 = rng.normal(scale=0.5, size=(n_classes, h2)); self.b3 = np.zeros(n_classes)\n    def tanh(self,z): return np.tanh(z)\n    def dtanh(self,z): return 1.0 - np.tanh(z)**2\n    def softmax(self,z):\n        z_max = np.max(z, axis=1, keepdims=True); exp = np.exp(z - z_max); return exp/exp.sum(axis=1, keepdims=True)\n    def forward(self,X):\n        z1 = X.dot(self.W1.T)+self.b1; a1 = self.tanh(z1)\n        z2 = a1.dot(self.W2.T)+self.b2; a2 = self.tanh(z2)\n        z3 = a2.dot(self.W3.T)+self.b3; a3 = self.softmax(z3)\n        return a3, (z1,a1,z2,a2,z3,a3)\n    def compute_loss(self,y_true_oh,y_pred):\n        eps=1e-9; y_pred=np.clip(y_pred,eps,1-eps); return -np.mean(np.sum(y_true_oh * np.log(y_pred), axis=1))\n    def backward(self,X,y_true_oh,cache):\n        z1,a1,z2,a2,z3,a3 = cache; N=X.shape[0]\n        dz3 = (a3 - y_true_oh)/N; dW3 = dz3.T.dot(a2); db3 = dz3.sum(axis=0)\n        da2 = dz3.dot(self.W3); dz2 = da2 * self.dtanh(z2); dW2 = dz2.T.dot(a1); db2 = dz2.sum(axis=0)\n        da1 = dz2.dot(self.W2); dz1 = da1 * self.dtanh(z1); dW1 = dz1.T.dot(X); db1 = dz1.sum(axis=0)\n        return dW1, db1, dW2, db2, dW3, db3\n\n# Treino\nmodel = DeeperMLP(n_inputs=4, hidden_sizes=(48,24), n_classes=3, lr=0.05, seed=1)\nepochs = 200\ntrain_losses = []\nfor epoch in range(1, epochs+1):\n    y_pred_train, cache = model.forward(X_train_s)\n    loss = model.compute_loss(y_train_oh, y_pred_train); train_losses.append(loss)\n    dW1, db1, dW2, db2, dW3, db3 = model.backward(X_train_s, y_train_oh, cache)\n    model.W1 -= model.lr * dW1; model.b1 -= model.lr * db1\n    model.W2 -= model.lr * dW2; model.b2 -= model.lr * db2\n    model.W3 -= model.lr * dW3; model.b3 -= model.lr * db3\n    if epoch==1 or epoch%50==0:\n        train_acc = np.mean(np.argmax(y_pred_train, axis=1) == y_train)\n        print(f\"Epoch 200 - loss: 0.4537 - train_acc: 0.8283\")\n\n# Avalia\u00e7\u00e3o\ny_pred_test_prob, _ = model.forward(X_test_s); y_pred_test = np.argmax(y_pred_test_prob, axis=1)\ntest_acc = accuracy_score(y_test, y_pred_test); cm = confusion_matrix(y_test, y_pred_test)\nprint(\"\\nAcur\u00e1cia no teste:\", test_acc); print(\"Matriz de confus\u00e3o:\\n\", cm)\n</code></pre>"},{"location":"Ex3_Mlp/Ex9/mlp4/#saidas-resultados","title":"Sa\u00eddas (resultados)","text":"<p>Progresso do treino: </p><pre><code>Epoch 001 - loss: 2.1389 - train_acc: 0.2925\nEpoch 050 - loss: 0.6005 - train_acc: 0.7558\nEpoch 100 - loss: 0.5206 - train_acc: 0.7933\nEpoch 150 - loss: 0.4807 - train_acc: 0.8158\nEpoch 200 - loss: 0.4537 - train_acc: 0.8283\n</code></pre><p></p> <p>Resultados no teste: </p><pre><code>Test accuracy: 0.8133\nConfusion matrix:\n[[82  9  9]\n [ 1 90  9]\n [ 6 22 72]]\n</code></pre><p></p>"},{"location":"Ex3_Mlp/Ex9/mlp4/#visualizacoes-geradas","title":"Visualiza\u00e7\u00f5es geradas","text":""},{"location":"Ex3_Mlp/Ex9/mlp4/#conclusao","title":"Conclus\u00e3o","text":"<p>A vers\u00e3o mais profunda do MLP (duas camadas ocultas) atingiu ~81.3% de acur\u00e1cia no conjunto de teste. A profundidade permitiu modelar estruturas mais complexas dos dados em rela\u00e7\u00e3o ao modelo de camada \u00fanica, e a curva de perda demonstra melhoria cont\u00ednua durante o treino.</p>"},{"location":"Ex4_Vae/vae/","title":"Exerc\u00edcio 10","text":""},{"location":"Ex4_Vae/vae/#exercicio-vae-variational-autoencoder","title":"Exercicio - VAE (Variational Autoencoder)","text":"<p>Este exerc\u00edcio realiza o treino, a arquitetura e os resultados do VAE treinado (entrada 784 -&gt; hidden 400 -&gt; latent 2), al\u00e9m da comparac\u00e3o com um Autoencoder padrao (AE).</p>"},{"location":"Ex4_Vae/vae/#arquitetura-e-hiperparametros","title":"Arquitetura e hiperparametros","text":"<p>Resumo:</p> <ul> <li>Input dimension: 784</li> <li>Hidden dimension: 400</li> <li>Latent dimension: 2</li> <li>Epocas: 50</li> <li>Total de parametros (reportado): 631188</li> </ul>"},{"location":"Ex4_Vae/vae/#treino-do-vae-logs-principais","title":"Treino do VAE - logs principais","text":"<p>Treino resumido por Epoch (valores medios por conjunto):</p> <pre><code>Epoch 1/50:\n  Train - Loss: 296.8121, Recon: 289.8587, KLD: 6.9534\n  Val   - Loss: 278.6441, Recon: 272.1831, KLD: 6.4611\n-&gt; Best model saved!\n\nEpoch 10/50:\n  Train - Loss: 263.9139, Recon: 257.5315, KLD: 6.3824\n  Val   - Loss: 264.6695, Recon: 258.2373, KLD: 6.4322\n-&gt; Best model saved!\n\nEpoch 25/50:\n  Train - Loss: 260.7063, Recon: 254.2618, KLD: 6.4445\n  Val   - Loss: 261.8023, Recon: 255.4813, KLD: 6.3210\n-&gt; Best model saved!\n\nEpoch 50/50:\n  Train - Loss: 258.6138, Recon: 252.0897, KLD: 6.5241\n  Val   - Loss: 260.5285, Recon: 253.7263, KLD: 6.8022\n</code></pre> <p>O log detalhado mostra que o modelo salvou o checkpoint \"best\" varias vezes ao longo do treino com melhora na valida\u00e7\u00e3o.</p>"},{"location":"Ex4_Vae/vae/#avaliacao-final-teste","title":"Avaliacao final (teste)","text":"<pre><code>Test Performance:\n  Total Loss: 261.0976\n  Reconstruction Loss: 254.7975\n  KLD Loss: 6.3000\n</code></pre>"},{"location":"Ex4_Vae/vae/#trechos-de-codigo-relevantes","title":"Trechos de codigo relevantes","text":"<p>Classe VAE :</p> <pre><code>class VAE(nn.Module):\n    def __init__(self, input_dim=784, hidden_dim=400, latent_dim=2):\n        super(VAE, self).__init__()\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n        self.fc_log_var = nn.Linear(hidden_dim, latent_dim)\n        self.fc3 = nn.Linear(latent_dim, hidden_dim)\n        self.fc4 = nn.Linear(hidden_dim, input_dim)\n\n    def encode(self, x):\n        h = F.relu(self.fc1(x))\n        mu = self.fc_mu(h)\n        log_var = self.fc_log_var(h)\n        return mu, log_var\n\n    def reparameterize(self, mu, log_var):\n        std = torch.exp(0.5 * log_var)\n        eps = torch.randn_like(std)\n        return mu + eps * std\n\n    def decode(self, z):\n        h = F.relu(self.fc3(z))\n        x_recon = torch.sigmoid(self.fc4(h))\n        return x_recon\n\n    def forward(self, x):\n        mu, log_var = self.encode(x)\n        z = self.reparameterize(mu, log_var)\n        x_recon = self.decode(z)\n        return x_recon, mu, log_var\n</code></pre> <p>Fun\u00e7\u00e3o de perda do VAE:</p> <pre><code>def vae_loss(x_recon, x, mu, log_var):\n    recon_loss = F.binary_cross_entropy(x_recon, x, reduction='sum')\n    kld_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n    total_loss = recon_loss + kld_loss\n    return total_loss, recon_loss, kld_loss\n</code></pre> <p>Treinamento:</p> <pre><code>for epoch in range(1, EPOCHS+1):\n    train_loss, train_recon, train_kld = train_vae(model, train_loader, optimizer, epoch)\n    val_loss, val_recon, val_kld = validate_vae(model, val_loader)\n</code></pre>"},{"location":"Ex4_Vae/vae/#visualizacoes-geradas","title":"Visualizacoes geradas","text":""},{"location":"Ex4_Vae/vae/#ae-latent-space-2d","title":"AE Latent Space (2D)","text":"<p>Proje\u00e7\u00e3o 2D do espa\u00e7o latente do Autoencoder padr\u00e3o. Cada ponto eh uma imagem do dataset codificada em 2 dimens\u00f5es; as cores representam as classes. Observa-se grupos parcialmente separados, por\u00e9m com sobreposi\u00e7\u00f5es mais evidentes, pois o AE n\u00e3o eh for\u00e7ado a organizar o latente como distribui\u00e7\u00e3o (sem termo KLD).</p>"},{"location":"Ex4_Vae/vae/#ae-training-curves","title":"AE Training Curves","text":"<p>Curva de perda de treino e valida\u00e7\u00e3o do AE ao longo das \u00e9pocas (apenas reconstru\u00e7\u00e3o). A queda r\u00e1pida inicial indica aprendizado das estruturas globais; a converg\u00eancia gradual mostra refinamento das reconstru\u00e7\u00f5es. Pequenas varia\u00e7\u00f5es na valida\u00e7\u00e3o sugerem leve oscila\u00e7\u00e3o, mas sem overfitting marcante.</p>"},{"location":"Ex4_Vae/vae/#vae-latent-space-2d","title":"VAE Latent Space (2D)","text":"<p>Proje\u00e7\u00e3o 2D do espa\u00e7o latente do VAE. Em compara\u00e7\u00e3o ao AE, os clusters tendem a ficar um pouco mais compactos e organizados gra\u00e7as ao termo KLD, que incentiva distribui\u00e7\u00f5es pr\u00f3ximas de Normal. Ainda h\u00e1 interse\u00e7\u00f5es entre classes com apar\u00eancia semelhante.</p>"},{"location":"Ex4_Vae/vae/#vae-training-curves","title":"VAE Training Curves","text":"<p>Quatro pain\u00e9is de curvas do VAE: (superior esquerdo) perda total; (superior direito) perda de reconstru\u00e7\u00e3o; (inferior esquerdo) KLD; (inferior direito) componentes de perda no treino. A reconstru\u00e7\u00e3o cai rapidamente, enquanto o KLD estabiliza entre ~6.3 e ~6.7 nats, equilibrando fidelidade da reconstru\u00e7\u00e3o e regulariza\u00e7\u00e3o do latente.</p>"},{"location":"Ex4_Vae/vae/#comparacao-com-autoencoder-padrao-ae","title":"Comparacao com Autoencoder padrao (AE)","text":"<p>Treino do AE (extra credit):</p> <pre><code>AE final (valores de exemplo):\n  Melhor Val Loss reportado ao longo do treino por checkpoints\n  Ex.: Epoch 1 AE Val Loss: 268.8340 (melhor salvo)\n  Ao final: Val Loss proximos de 253..256 conforme logs\n</code></pre> <p>O AE apresenta reconstru\u00e7\u00f5es competitivas e perda de reconstru\u00e7\u00e3o ligeiramente menor que a parte de recon do VAE (por constru\u00e7\u00e3o o VAE soma o termo KLD), mas o VAE tamb\u00e9m aprende a distribui\u00e7\u00e3o latente e permite amostragem e gera\u00e7\u00e3o de novas imagens.</p>"}]}